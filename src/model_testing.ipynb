{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    # Initialize models\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    gbm = GradientBoostingClassifier(n_estimators=100)\n",
    "    \n",
    "    # Train models\n",
    "    rf.fit(X_train, y_train)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    for name, model in [('Random Forest', rf), ('GBM', gbm)]:\n",
    "        preds = model.predict(X_test)\n",
    "        print(f\"{name} Report:\")\n",
    "        print(classification_report(y_test, preds))\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovo')}\")\n",
    "    \n",
    "    # Explain model\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    \n",
    "    # Save best model\n",
    "    joblib.dump(rf, 'models/mental_health_model.pkl')\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training both models\n",
    "model_metrics = []\n",
    "\n",
    "for name, model in [('Random Forest', rf), ('XGBoost', xgb)]:\n",
    "    preds = model.predict(X_test)\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds, average='weighted'),\n",
    "        'Recall': recall_score(y_test, preds, average='weighted'),\n",
    "        'F1': f1_score(y_test, preds, average='weighted'),\n",
    "        'ROC-AUC': roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovo')\n",
    "    }\n",
    "    model_metrics.append(metrics)\n",
    "\n",
    "# Create comparison table\n",
    "results_df = pd.DataFrame(model_metrics)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
